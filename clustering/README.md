# Social Media Persona Data Processing

This directory contains scripts for processing and clustering social media data from Bluesky to identify user personas.

## Overview

The data processing pipeline:
1. Parses raw Bluesky data files
2. Extracts user posts, replies, likes, reposts, and follows
3. Filter out users without enough english posts
4. Computes embeddings for text content using a multilingual transformer model
5. Clusters users based on content similarity
6. Outputs structured data for each cluster

For additional details see https://docs.google.com/document/d/1O5Ntn8QanFF451hYyyveZo74rWHL4tIKVCr4TaB8BSQ/edit?tab=t.3a8j1itym6zh#heading=h.rnah75j9h0ng

## Scripts

### `cluster_tweets_raw.py`

Main processing script that handles the entire pipeline from raw data to clustered output.

```
python cluster_tweets_raw.py --auto-cluster --start-date 1 --end-date 16 --n-workers 3
```

#### Arguments

- `--auto-cluster`: Use automatic clustering to determine optimal number of clusters
- `--n-clusters`: Number of clusters if not using auto-clustering (default: 10)
- `--similarity-threshold`: Similarity threshold for ignored content (default: 0.7)
- `--start-date`: Start date to process (day of month)
- `--end-date`: End date to process (day of month)
- `--force-parse`: Force parsing of files even if cache exists
- `--n-workers`: Number of workers to use for multiprocessing (default: 5)
- `--add-ignored-messages`: Estimates which messages were seen and then ignored. Adds these to the dataset
- `--cap-ignored-messages`: Cap the number of ignored messages per user (default: 1000)

#### Before using

Make sure you set the constants at the beginning of the script to the right values (path of dataset, output, caches...)

### `get_cluster_stats.py`

This script analyzes the output clusters generated by `cluster_tweets_raw.py`. It calculates various statistics for each cluster, such as the number of users, total actions, counts for specific actions (posts, replies, likes, etc.). It also identifies the top words using TF-IDF, computes representative medoid posts based on embeddings, and extracts potential topics using Latent Dirichlet Allocation (LDA). The results are printed to the console, including summary tables in Markdown format.

### `job_script.sh`

SLURM job script for running the clustering process on a computing cluster.

```
sbatch job_script.sh
```


## Dependencies

The scripts require the following Python packages:
- transformers
- sklearn
- torch
- numpy
- tqdm

Install dependencies with:
```
pip install -r requirements.txt
```

## Output

The script generates the following outputs in the specified output directory:
- `cluster_[id].jsonl`: Data files for each cluster containing:
    - Conversation chains
    - Like actions
    - Repost actions
    - Follow actions
    - Ignored content
- `user_clusters.json`: Mapping of users to their assigned clusters

## Cache Files

To speed up processing, the script maintains cache files:
- `embedding_cache.pkl`: Cache of text embeddings
- `user_data_cache.pkl`: Cache of processed user data

## Hardware Requirements

- Processing raw data requires significant RAM. Consider the size of your dataset when choosing it
- GPU acceleration is highly recommended for computing embeddings
- Multi-core processing is supported for faster data parsing