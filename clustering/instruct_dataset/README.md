# Instruction-Following Dataset Generation (`generate_instructions.py`)

This document describes the `generate_instructions.py` script, which is used to generate an instruction-following dataset from clustered social media data. The script processes conversation chains from these clusters and mixes them with data from the Databricks Dolly 15k dataset.

## Overview

The `generate_instructions.py` script aims to create a diverse set of instruction-response pairs based on social media conversations. These pairs can then be used to fine-tune language models for instruction-following tasks relevant to social media interactions.

Key functionalities include:
- Processing `jsonl` files containing clustered social media conversation chains.
- Formatting conversation histories for context.
- Generating various types of instructions:
    - **Needle in a Haystack**: Finding a specific message within a conversation history.
    - **Summarization**: Summarizing a given conversation.
    - **Topic Generation**: Creating a social media post about a given topic.
    - **Keyword Generation**: Crafting a social media post containing specific keywords.
    - **Completion**: Completing a partially written social media post.
    - **Paraphrasing**: Rewriting a given post in different words.
    - **Voting Intention**: (If applicable) Inferring voting intention based on post history.
- Mixing generated instructions with a specified fraction of the Databricks Dolly 15k dataset.

## Prerequisites

1.  **Python Environment**: Ensure you have a Python environment with the necessary libraries installed. Key libraries include `argparse`, `random`, `os`, `getpass`, and `json`.
2.  **Clustered Data**: Input data should be in the form of `cluster_*.jsonl` files, typically generated by a preceding clustering pipeline (see `bluesky_blueprint/clustering/README.md`). Each file should contain conversation chains.
3.  **Databricks Dolly 15k Dataset**: You need the `databricks-dolly-15k.jsonl` file.
    -   Download it from [Hugging Face Datasets: databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k).
    -   The default expected path is `~/bluesky_blueprint/scratch/databricks-dolly-15k.jsonl`, but this can be configured via arguments.

## How to Run

Navigate to the directory containing `generate_instructions.py` (i.e., `bluesky_blueprint/clustering/instruct_dataset/`) and run the script from your terminal.

```bash
python generate_instructions.py [ARGUMENTS]
```

### Arguments

The script accepts the following command-line arguments:

-   `--input_dir`: (String) Directory containing the input `cluster_*.jsonl` files.
    -   Default: `~/bluesky_blueprint/scratch/pii_removed/processed_25_clusters_hashed`
-   `--output_dir`: (String) Directory where the generated instruction files will be saved.
    -   Default: `~/bluesky_blueprint/scratch/instruction_following`
-   `--dolly_path`: (String) Path to the `databricks-dolly-15k.jsonl` file.
    -   Default: `~/bluesky_blueprint/scratch/databricks-dolly-15k.jsonl`
-   `--max_chains_per_cluster`: (Integer) Maximum number of conversation chains to process from each cluster file.
    -   Default: `10000`
-   `--dolly_mix_fraction`: (Float) Fraction of instructions from the Dolly dataset to mix into the output.
    -   Default: `0.2` (i.e., 20%)
-   `--seed`: (Integer) Random seed for reproducibility.
    -   Default: `42`

**Example Usage:**

```bash
python generate_instructions.py \
    --input_dir ~/bluesky_blueprint/scratch/my_clusters \
    --output_dir ~/bluesky_blueprint/scratch/my_instructions \
    --dolly_path ~/downloads/databricks-dolly-15k.jsonl \
    --max_chains_per_cluster 500 \
    --dolly_mix_fraction 0.25 \
    --seed 123
```

## Input Data Format

-   **Cluster Files (`cluster_*.jsonl`)**: These files are expected to be line-delimited JSON, where each line represents a conversation chain (a list of tweet-like objects). Each tweet object should ideally contain fields like `text`, `user_id`, and timestamp information for proper processing. The `extract_text` function in the script details how text is pulled from tweet objects.
-   **Dolly Dataset (`databricks-dolly-15k.jsonl`)**: This is a standard JSONL file where each line is a JSON object containing fields like "instruction", "context", and "response".

## Output

The script generates instruction-following data, typically saved as `jsonl` files in the specified `--output_dir`. Each line in the output file will be a JSON object representing an instruction-response pair, formatted as a list of messages suitable for chat-based models:

```json
{"messages": [{"role": "system", "content": "<Generated or Dolly Instruction>"}, {"role": "assistant", "content": "<Generated or Dolly Response>"}]}
```
Or, for some specific instruction types, it might directly be:
```json
[{"role": "system", "content": "<Instruction>"}, {"role": "assistant", "content": "<Response>"}]
```
The exact structure depends on the processing within the `main()` function of the script. The output files are intended for use in fine-tuning language models.

## Customization

-   **Instruction Types**: You can modify or add new instruction generation functions (e.g., `generate_needle_haystack`, `generate_summarization_prompt`) to create different types of tasks.
-   **Prompt Templates**: The various `*_instructions` lists within the script (e.g., `needle_haystack_instructions`, `summarization_instructions`) contain templates for prompts. These can be easily customized or expanded.