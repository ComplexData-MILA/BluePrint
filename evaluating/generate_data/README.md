# Data Generation for Evaluation

This directory contains scripts and utilities for generating text data using language models, which can then be used for evaluating model performance. It supports generation via the OpenAI Batch API and also using locally fine-tuned Hugging Face models.

## Overview

The primary scripts and their purposes are:

-   **`create_batchinput.py`**: Prepares the `batchinput.jsonl` file, which is used as input for OpenAI batch processing. It takes prompts or other structured data and formats them into the required JSONL structure for the OpenAI Batch API.
-   **`batch_utils.py`**: A command-line utility for interacting with the OpenAI Batch API. It allows uploading batch input files, creating and managing batch jobs, and downloading their results.
-   **`generate_data_finetuned.py`**: Generates text data using a fine-tuned Hugging Face transformer model (with LoRA adapters). This script is used for evaluating models that you have trained yourself.
-   **`reformat_results.py`**: Processes or reformats the output files generated by either the OpenAI Batch API (via `batch_utils.py`) or `generate_data_finetuned.py`. This involves structuring the data for easier analysis and for input into metrics computation scripts.
-   **`job_script.sh`**: A SLURM job script for running data generation tasks on a cluster, focused on `generate_data_finetuned.py` for more intensive, local model generation.

---

## 1. `create_batchinput.py` - Creating OpenAI Batch Input Files

This script is responsible for generating the `batchinput.jsonl` file required by `batch_utils.py` for OpenAI Batch API requests.

### Purpose
- Takes a source of prompts or structured data (e.g., evaluation dataset).
- Formats each prompt into the JSONL structure expected by the OpenAI Batch API (typically for the `/v1/chat/completions` endpoint).
- Outputs the `batchinput.jsonl` file.

### Prerequisites
- Python environment with necessary libraries (see `requirements.txt`).
- Input data/prompts for generation.

### How to Run
```bash
python create_batchinput.py --input_file <path_to_prompts_data> --output_file batchinput.jsonl --model_name <openai_model_to_use>
```

**Key Arguments (example, actual arguments may vary):**
- `--input_file`: Path to the file containing the source prompts or data.
- `--output_file`: Path to save the generated `batchinput.jsonl` file. Default: `batchinput.jsonl`.
- `--model_name`: The OpenAI model to specify in the batch requests (e.g., `gpt-4-turbo`).

---

## 2. OpenAI Batch API Utility (`batch_utils.py`)

This Python script provides a command-line interface for interacting with the OpenAI Batch API. It allows users to upload batch input files, create and manage batch jobs, and download their results.

### Prerequisites

1.  **Python Environment**:
    *   Ensure you have Python installed.
    *   Install the required `openai` library:
        ```bash
        pip install openai
        ```
    *   The script also uses the `argparse` and `os` standard libraries.

2.  **OpenAI API Key**:
    *   You need a valid OpenAI API key.
    *   Create a file named `openai.tok` in this directory (`~/bluesky_blueprint/evaluating/generate_data/`).
    *   Place your OpenAI API key as the sole content of this `openai.tok` file.

3.  **Input File**:
    *   Prepare a batch input file in JSONL format (e.g., generated by `create_batchinput.py`). Each line should be a valid request object for the OpenAI API endpoint you intend to use (e.g., `/v1/chat/completions`).
    *   The default input filename is `batchinput.jsonl`.

### How to Run

Navigate to this directory and run the script from your terminal:

```bash
python batch_utils.py [ARGUMENTS]
```

### Command-Line Arguments

The script accepts the following arguments:

*   `--run`: (Flag) If present, uploads the input file specified by `--file` and creates a new batch job. The ID of the created batch is saved to `last_batch_id.tok`.
*   `--file <filepath>`: (String) Path to the JSONL input file containing the batch requests.
    *   Default: `batchinput.jsonl`
*   `--download <output_path>`: (String) If specified, attempts to download the output file for a given batch ID to the `<output_path>`. Requires `--id` or a previous `--run` (to use the last saved ID). If the batch resulted in errors, an additional `.errors.jsonl` file will be downloaded.
*   `--check`: (Flag) If present, checks and prints the status of the batch job specified by `--id` (or the last run ID).
*   `--cancel`: (Flag) If present, attempts to cancel the batch job specified by `--id` (or the last run ID).
*   `--id <batch_id>`: (String) The ID of the batch to check, cancel, or download output for. If omitted for these operations, the script will try to use the ID from `last_batch_id.tok` (saved from a previous `--run`).

If no action arguments (`--run`, `--check`, `--cancel`, `--download`) are provided, the script will list your recent batches.

### Examples

**1. Run a new batch job (assuming `batchinput.jsonl` exists or was created by `create_batchinput.py`):**
```bash
python batch_utils.py --run
```

**2. Check the status of the last run batch:**
```bash
python batch_utils.py --check
```

**3. Download the output of the last run batch:**
```bash
python batch_utils.py --download batch_results.jsonl
```

### Input File Format (`batchinput.jsonl`)

The input file (e.g., `batchinput.jsonl`) must be a JSONL file where each line is a JSON object representing a single API request. For chat completions, each line would look like this:

```json
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4-turbo", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello world!"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4-turbo", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the weather like today?"}]}}
```
Refer to the [OpenAI Batch API documentation](https://platform.openai.com/docs/api-reference/batch) for detailed information on formatting requests.

### Output (`batch_utils.py`)

-   **Batch Job Creation (`--run`)**: Prints the batch input file metadata and the created batch job metadata. Saves the batch ID to `last_batch_id.tok`.
-   **Status Check (`--check`)**: Prints the status object of the specified batch.
-   **Cancellation (`--cancel`)**: Prints a confirmation message if the cancellation request was sent.
-   **Download (`--download`)**: Saves the batch output to the specified file path. If errors occurred, an additional file with a `.errors.jsonl` suffix is saved. Prints messages indicating success or failure.
-   **Listing Batches**: Prints a list of recent batch jobs.

### Key Files for OpenAI Batch Processing

-   `create_batchinput.py`: Script to generate `batchinput.jsonl`.
-   `batch_utils.py`: The main script for OpenAI Batch API interaction.
-   `openai.tok`: Stores your OpenAI API key (must be created by the user).
-   `batchinput.jsonl` (or user-specified via `--file`): The input file for batch requests.
-   `last_batch_id.tok`: Stores the ID of the most recently created batch job by `--run`. This file is automatically created and updated.

---

## 3. `generate_data_finetuned.py` - Generating Data with Fine-tuned Models

This script is used to generate text outputs using a locally fine-tuned Hugging Face transformer model, typically one with LoRA adapters trained in the `bluesky_blueprint/finetuning/` stage.

### Purpose
- Load a base Hugging Face model and apply fine-tuned LoRA adapters.
- Take an input file of prompts or evaluation data.
- Generate responses/continuations for each input.
- Save the generated outputs to a specified file.

### Prerequisites
- Python environment with `transformers`, `peft`, `torch`, etc. (see `requirements.txt`).
- A fine-tuned model (base model + LoRA adapter path).
- An input file containing prompts or data for generation (e.g., JSONL).

### How to Run
```bash
python generate_data_finetuned.py \
    --model_name_or_path <path_or_hf_name_of_base_model> \
    --adapter_path <path_to_your_lora_adapters_directory> \
    --input_file <path_to_evaluation_prompts.jsonl> \
    --output_file <path_to_save_generated_outputs.jsonl> \
    --batch_size <inference_batch_size> \
    # Other generation parameters like max_length, temperature, etc.
```

**Key Arguments (example, actual arguments may vary):**
- `--model_name_or_path`: Identifier for the base Hugging Face model.
- `--adapter_path`: Path to the directory containing the trained LoRA adapter (`adapter_model.safetensors` and `adapter_config.json`).
- `--input_file`: Path to the input data (e.g., JSONL file with a "prompt" field).
- `--output_file`: Path to save the generated outputs.
- `--batch_size`: Batch size for inference.

### SLURM Execution (`job_script.sh` for fine-tuned generation)

The `job_script.sh` in this directory is configured to run `generate_data_finetuned.py` on a SLURM cluster.

- **Modify** SLURM directives (account, resources, array tasks if applicable) and script parameters within `job_script.sh` to match your model, data paths, and desired generation settings.
- **Submit the job**:
  ```bash
  sbatch job_script.sh
  ```

---

## 4. `reformat_results.py` - Reformatting Generation Outputs

This script is used to process and reformat the output files obtained from either `batch_utils.py` (OpenAI results) or `generate_data_finetuned.py` (local model results).

### Purpose
- Parse the raw output files (which might be JSONL with nested structures).
- Extract relevant information (e.g., generated text, custom IDs).
- Transform the data into a more convenient format for analysis or for input into metric computation scripts (e.g., a simple JSONL with `id` and `generated_text` fields).

### Prerequisites
- Python environment.
- Generated output files from previous steps.

### How to Run
```bash
python reformat_results.py --input_file <path_to_raw_output.jsonl> --output_file <path_to_reformatted_output.jsonl> --source_type <openai_batch_or_finetuned>
```

**Key Arguments (example, actual arguments may vary):**
- `--input_file`: Path to the raw generated output file.
- `--output_file`: Path to save the reformatted data.
- `--source_type`: Indicates the source of the input file to apply correct parsing logic (e.g., `openai` or `finetuned`).

---

## General Workflow

1.  **For OpenAI Generation**:
    a.  (Optional) Use `create_batchinput.py` to prepare `batchinput.jsonl` from your prompts.
    b.  Use `batch_utils.py --run` to submit the batch job to OpenAI.
    c.  Use `batch_utils.py --check` to monitor its status.
    d.  Once completed, use `batch_utils.py --download` to get the results.
    e.  (Optional) Use `reformat_results.py` to process the downloaded OpenAI results.

2.  **For Fine-tuned Model Generation**:
    a.  Prepare your input prompts file.
    b.  Use `generate_data_finetuned.py` (directly or via `job_script.sh` on SLURM) to generate outputs using your model.
    c.  (Optional) Use `reformat_results.py` to process the generated outputs.

3.  The processed outputs can then be used as input for evaluation metric scripts in the parent `evaluating` directory.

## Dependencies

Refer to `requirements.txt` for a list of Python dependencies. Ensure your OpenAI API key is correctly set up in `openai.tok` if using `batch_utils.py`.
