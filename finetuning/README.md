https://arxiv.org/pdf/2309.09530 maintain instruction-following capabilities
after finetuning by including "reading comprehension" questions in the training
dataset.

If there is no principled way to get answers, I could generate answers from
strong models and use them in the fine-tuning, kind of distillation style.

- What is the n-th message from your post history? More generally needle in a haystack style questions
- Summarize the following messages [get strong LLM to provide answer to use in training]
- Generate a post about the following topic [get real post with said topic]
- Generate a sentence/post that contains the following words (ideally make the words domain specific)
- Complete the following post/thread
- I need to find a way to query for voting opinion. Maybe give someone's post history to
a strong LLM, ask it to say who they'd vote for?
- Paraphrase this post [answer generated by strong llm]
- mix with general instructions (find some instruction tuning dataset online and sprinkle samples from that during training)
https://huggingface.co/datasets/databricks/databricks-dolly-15k/blob/main/databricks-dolly-15k.jsonl this dataset looks alright,
handmade.

I should write multiple variants of each prompt (using some strong llm).

https://frq.gouv.qc.ca/en/program/frqnt-2024-2025-masters-training-scholarships/#eligibility